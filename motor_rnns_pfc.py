# -*- coding: utf-8 -*-
"""motor_RNNs

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1psQV5sK5Ox0GU5uAzLaZ4urxTAFMs0BP

# Motor RNNs

contact: steeve.laquitaine@epfl.ch

Heavily inspired from [Feulner & Clopath, 2021](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008621)

## Setup
"""

# import python packages
import os
import numpy as np
import sklearn.linear_model as lm
import matplotlib.pyplot as plt
from IPython import display

# @title Set the project path
proj_path = "C:/Users/arrac/Desktop/MotorRNN/proj_rnn/"
if not os.path.exists(proj_path):
  os.makedirs(proj_path)

# set the directories where the results will be saved
savedir = os.path.join(proj_path, 'data/fig2/')
if not os.path.exists(savedir):
  os.makedirs(savedir)

"""## Utils

### Network
"""

# @title RNN encoder
# @title RNN encoder
class RNN(object):
    """
    Class implementing a recurrent network (not following Dale's law).

    Parameters:
    -----------
    * N: number of neurons
    * N_in: how many inputs can the network have
    * N_out: how many neurons are recorded by external device
    * g: recurrent coupling strength
    * p: connection probability
    * tau: neuron time constant
    * dt: set dt for simulation
    * delta: defines initial learning rate for FORCE
    * P_plastic: how many neurons are plastic in the recurrent network
    """
    def __init__(self, N=800, g=1.5, p=0.1, tau=0.1, dt=0.01,
                 N_in=6):
        # set parameters
        self.N = N
        self.g = g
        self.p = p
        self.K = int(p*N)
        self.tau = tau
        self.dt = dt

        # create recurrent W
        mask = np.random.rand(self.N,self.N)<self.p
        np.fill_diagonal(mask,np.zeros(self.N))
        self.mask = mask
        self.W = self.g / np.sqrt(self.K) * np.random.randn(self.N,self.N) * mask

        # create Win and Wout
        self._N_in = N_in
        self.W_in = (np.random.rand(self.N, self._N_in)-0.5)*2.

    @property
    def N_in(self):
        return self._N_in

    @N_in.setter
    def N_in(self, value):
        self._N_in = value
        self.W_in = (np.random.rand(self.N, self._N_in)-0.5)*2.

    def save(self,filename):
        np.savez(
            filename,
            N = self.N,
            K = self.K,
            tau = self.tau,
            g = self.g,
            p = self.p,
            dt = self.dt,
            W_in = self.W_in,
            W = self.W,
            N_in = self._N_in,
        )

    def load(self,filename):
        net = np.load(filename+'.npz')
        self.N = int(net['N'])
        self.dt = float(net['dt'])
        self.K = int(net['K'])
        self.tau = float(net['tau'])
        self.g = float(net['g'])
        self.p = float(net['p'])
        self.W_in = net['W_in']
        self.W = net['W']
        self._N_in = int(net['N_in'])

    def update_activation(self):
        self.z = np.tanh(self.r)

    def update_neurons(self,ext):
        self.r = self.r + self.dt/self.tau * \
             (-self.r + np.dot(self.W, self.z) + np.dot(self.W_in,ext))

        self.update_activation()
    
    def update_activation_gated(self, gating_signal):
        """Applies a multiplicative gate to the neuron's output activation."""
        self.z = gating_signal * self.z

    def simulate(self, T, ext=None, r0=None):

        # define time
        time = np.arange(0,T,self.dt)
        tsteps = int(T/self.dt)

        # create input in case no input is given
        if ext is None:
            ext = np.zeros((tsteps,self.N_in))

        # check if input has the right shape
        if ext.shape[0]!=tsteps or ext.shape[1]!=self.N_in:
            print('ERROR: stimulus shape should be (time x number of input nodes)')
            return

        # set initial condition
        if r0 is None:
            self.r = (np.random.rand(self.N)-0.5)*2.
        else:
            self.r = r0
        self.update_activation()

        # start simulation
        record_r = np.zeros((tsteps,self.N))
        record_r[0,:] = self.r
        for i in range(1,tsteps):
            self.update_neurons(ext=ext[i])
            # store activity
            record_r[i,:] = self.r
        return time, record_r, np.tanh(record_r)

    def relearn(self, trials, ext, ntstart, decoder, feedback, target, delta=1.,
                wplastic=None):
        """
        Args
          self.z: RNN network's activation
          ext (np.array): stimuli (n_targets, n timesteps, n_targets)
          decoder (np.array): (N units, 2d coordinates) decoder weights
          feedback (np.array): (N units, 2d coordinates) feedback weights
          target: (n_targets, N timesteps, 2d coordinates) target coordinates

        Returns:

          loss (np.array): loss by trial
        """
        # get number of timesteps within trial
        tsteps = ext.shape[1]

        # set up learning
        if wplastic is None:
            self.W_plastic = [np.where(self.W[i,:]!=0)[0] for i in range(self.N)]
        else:
            self.W_plastic = wplastic
        self.P = [1./delta*np.eye(len(self.W_plastic[i])) for i in range(len(self.W_plastic))]

        # create n trials of target indices chosen from 0 to 5
        order = np.random.choice(range(ext.shape[0]), trials, replace=True)

        # initialize calculated loss per trial
        record_loss = np.zeros(trials)

        # loop over trials
        for t in range(trials):

            # initialize loss
            loss = 0.
            self.r = (np.random.rand(self.N)-0.5)*2.
            self.update_activation()

            # loop over time
            for i in range(1,tsteps):

                # update units
                self.update_neurons(ext=ext[order[t],i])

                # learn
                if i > ntstart and i%2==0:

                    # decode network's predicted
                    # target coordinates
                    c = decoder @ self.z

                    # calculate prediction error between
                    # decoded and true target coordinates (2,)
                    errc = c - target[order[t], i]

                    # calculate the error update assigned to each weight
                    err1 = feedback @ errc

                    # calculate loss
                    loss += np.mean(err1**2)

                    # update plastic recurrent weights
                    for j in range(self.N):
                        z_plastic = self.z[self.W_plastic[j]]
                        pz = np.dot(self.P[j], z_plastic)
                        norm = (1. + np.dot(z_plastic.T,  pz))
                        self.P[j] -= np.outer(pz, pz)/norm

                        # use error-transformed feedbacks to update
                        # plastic weights
                        self.W[j, self.W_plastic[j]] -= err1[j] * pz / norm

            # tape loss
            record_loss[t] = loss
            print('Loss in Trial %d is %.5f'%(t+1,loss))
        return record_loss

    def calculate_manifold(self, trials, ext, ntstart):
        tsteps = ext.shape[1]
        T = self.dt*tsteps
        points = (tsteps-ntstart)
        activity = np.zeros((points*trials,self.N))
        order = np.random.choice(range(ext.shape[0]),trials,replace=True)
        for t in range(trials):
            time, r, z = self.simulate(T,ext[order[t]])
            activity[t*points:(t+1)*points,:] = z[ntstart:,:]
        cov = np.cov(activity.T)
        ev,evec = np.linalg.eig(cov)
        pr = np.round(np.sum(ev.real)**2/np.sum(ev.real**2)).astype(int)
        xi = activity @ evec.real
        return activity,cov,ev.real,evec.real,pr,xi,order

def save_RNN(network, savedir:str):
  """write RNN object and weights in savedir
  """
  network.save(savedir+'network')
  np.save(savedir+'W_initial', network.W)

def save_RNN_sinewave(network, savedir:str):
  """write RNN sinewave object and weights in savedir
  """
  network.save(savedir + 'network_sinewave')
  np.save(savedir + 'W_initial_sinewave', network.W)

# @title BCI decoders
def train_reaching_decoder(inputP, target, order, n_output_units:int=2):
    """train the decoder to perform the six-cue
    motor reaching task
    """
    # initialize predictor neural activity
    X = np.zeros((inputP.shape[0]*inputP.shape[1], inputP.shape[-1]))

    # initialize predicted target
    Y = np.zeros((inputP.shape[0]*inputP.shape[1], n_output_units))

    # fill up
    for j in range(inputP.shape[0]):
        X[j*inputP.shape[1]:(j+1)*inputP.shape[1],:] = inputP[j]
        Y[j*inputP.shape[1]:(j+1)*inputP.shape[1],:] = target[order[j]]

    # regress target against neural activity
    reg = lm.LinearRegression()
    reg.fit(X,Y)

    # make predictions
    y = reg.predict(X)
    mse = np.mean((y-Y)**2)
    return reg.coef_, mse

def create_reaching_task_decoder(reaching_network, n_output_units:int=2):
  """create feedforward decoder from RNN to (x,y) output units
  for learning (random weights)"""

  # set parameters
  SCALE = 0.04
  DENOM = 0.2

  # create random weights
  reaching_decoder = np.random.randn(n_output_units, reaching_network.N)
  initial_decoder_fac = SCALE * (target_max / DENOM)

  # normalize decoder matrix
  reaching_decoder *= (initial_decoder_fac / np.linalg.norm(reaching_decoder))
  return reaching_decoder

def train_force_exertion_decoder(inputP, target, order, n_output:int=1):
    """train the decoder to perform the force exertion
    motor task. The network must apply force at
    oscillating amplitude (following a sinewave function
    of time)
    """

    # initialize predictor neural activity
    X = np.zeros((inputP.shape[0]*inputP.shape[1], inputP.shape[-1]))

    # initialize predicted target
    Y = np.zeros((inputP.shape[0]*inputP.shape[1], n_output))

    # fill up
    for j in range(inputP.shape[0]):
        X[j*inputP.shape[1]:(j+1)*inputP.shape[1],:] = inputP[j]
        Y[j*inputP.shape[1]:(j+1)*inputP.shape[1],:] = target[order[j]]

    # regress target against neural activity
    reg = lm.LinearRegression()
    reg.fit(X, Y)

    # make predictions
    y = reg.predict(X)
    mse = np.mean((y-Y)**2)
    return reg.coef_, mse

# @title Feedback weights
def get_feedback_weights(decoder):
  """calculate feedback weights from (x,y) output units back to RNN
  as the matrix inverse of the feedforward decoder weights from the RNN to
  the output units"""
  return np.linalg.pinv(decoder)

# @title Loss function
def get_cost(result, target, order):
  cost = 0
  for j in range(result.shape[0]):
    error = result[j, :, :] - target[order[j], :, :]
    cost += np.mean(error**2)
  return cost

# @title Get the manifold
def get_manifold(network):

  # calculate the manifold
  activity, cov, ev, evec, pr, xi, order = network.calculate_manifold(trials=manifold_trials, ext=stimulus, ntstart=pulse_length)

  # reshape the activity
  activity_reshaped = activity.reshape(manifold_trials, -1, network.N)
  xi2 = xi.reshape(manifold_trials, -1, network.N)
  return {"xi2":xi2, "order":order, "xi":xi, "cov":cov, "ev":ev, "evec":evec, "pr":pr,"activity":activity, "activity_reshaped":activity_reshaped}

def save_reaching_manifold(data, T):
  dic = {'manifold': {'original': data['manifold']}, 'perturbations': {'transformed':T}}
  np.save(savedir + 'reaching_relearning_results', dic)

def transform_reaching(reaching_network, manifold_out, W_bci4, n_output_units:int=2):

  P = manifold_out["evec"].real.T
  D = np.zeros((2, reaching_network.N))
  D[:,:reduced_dim] = W_bci4
  transformed = D @ P
  # result = manifold_out["activity_reshaped"] @ transformed.T
  # cost = get_cost(result, target[:,pulse_length:,:], manifold_out["order"])
  return transformed

# @title simulation
def simulate_reaching(savdir, dt):

  # set plot parameters
  COL_ORIG = 'k'
  ALPHA = 1

  # load velocity data
  data = np.load(savdir + 'reaching_relearning_results.npy', allow_pickle=True).item()
  activity = data['manifold']['original']['activity2']
  o_original = activity @ data['perturbations']['transformed'].T

  # reconstruct trajectories from velocities
  pos_original = np.zeros(o_original.shape)
  for j in range(activity.shape[1]):
      pos_original[:,j,:] = pos_original[:,j-1,:] + o_original[:,j,:]*dt

  # plot trajectories
  plt.figure(figsize=(15,10), dpi=96)
  plt.subplot(2,3,3)
  for j in range(manifold_trials):
      plt.plot(pos_original[j,:,0], pos_original[j,:,1], COL_ORIG, alpha=ALPHA);
  plt.title('simulated reaching');
  plt.xlabel('x-position on screen');
  plt.ylabel('y-position on screen');
  
def plot_variance_explained(evals, num_pc=10):
    var_explained = (evals) / np.sum(evals)
    csum_var_explained = np.cumsum(evals) / np.sum(evals)
    fig, ax = plt.subplots(figsize=(6, 6))
    ax.bar(np.arange(1, len(var_explained[:20]) + 1), var_explained[:20])
    ax.scatter(np.arange(1, len(csum_var_explained[:20]) + 1), csum_var_explained[:20], c='r')
    ax.plot(np.arange(1, len(csum_var_explained[:20]) + 1), csum_var_explained[:20],
             '--k')
    ax.set_xticks(np.arange(1, 21))
    ax.set_xlabel('Number of components')
    ax.set_ylabel('Variance explained')
    # plt.grid('on')
    plt.show()
    return fig
"""### Task"""

# @title "reaching" task
def create_reaching_task_stimuli(tsteps:int, pulse_steps:int, n_targets:int=6, amplitude:float=1., twod:bool=False):
    """create the set of stimuli, which we sample from at each trial

    Args:
      tsteps (int):
      pulse_steps (int):
      n_targets (int):
      amplitude (float):
      twod (bool):

    Returns:
      (np.array): array of (n_targets, pulse_steps, n_targets) stimuli
    """

    # create stimulus
    stimulus = np.zeros((n_targets, tsteps, n_targets))
    if twod:
        phis = np.linspace(0,2*np.pi,targets,endpoint=False)
        for j in range(stimulus.shape[0]):
            stimulus[j,:pulse_length,0] = amplitude*np.cos(phis[j])
            stimulus[j,:pulse_length,1] = amplitude*np.sin(phis[j])
            stimulus[j,:pulse_length,2:] = 0
    else:
        for j in range(n_targets):
            stimulus[j,:pulse_steps,j] = amplitude
    return stimulus

def create_reaching_task_targets(tsteps, pulse_steps, n_targets:int=6, stype='constant', target_max:float=0.2):
    """create the set of target coordinates (6 by default) that the network
    must reach before the end of a trial. The network starts from the center of
    the computer screen (coordinate: (0,0))
    """
    # create target trajectories
    phis = np.linspace(0, 2*np.pi, n_targets, endpoint=False)
    rs = np.zeros(tsteps)

    # define each target's x and y coordinate
    rs[pulse_steps:] = np.ones(tsteps-pulse_steps)*target_max
    traj = np.zeros((n_targets,tsteps,2))
    for j in range(n_targets):

        # create x-coordinate on screen
        traj[j,:,0] = rs*np.cos(phis[j])

        # create y-coordinate on screen
        traj[j,:,1] = rs*np.sin(phis[j])
    return traj

def plot_reaching_task_stimuli(stimulus, n_targets:int, tsteps:int, T:int):

  # plot target cue with "pulse_steps" duration
  # at the beginning of each trial
  stimulus_set = np.arange(0, n_targets,1)

  fig, axes = plt.subplots(n_targets, 1, figsize=(30,9))

  for target in stimulus_set:
    axes[target].imshow(stimulus[target,:,:].T, aspect=10, cmap="binary");

    # legend
    axes[target].set_yticks(stimulus_set)
    axes[target].set_yticklabels(stimulus_set, fontsize=9)
    axes[target].set_xticks([0, tsteps])
    axes[target].set_xticklabels([0, T])
    axes[target].set_ylabel("possible targets", fontsize=9)

  axes[-1].set_xlabel("time within a trial (secs)")

  fig.tight_layout()

  print("stimuli:")
  print(f"-a set of {stimulus.shape[0]} possible trial stimuli (panels)")
  print(f"-{stimulus.shape[1]} timesteps within a trial stimulus")
  print(f"-{stimulus.shape[2]} possible cued target in a trial stimulus")

def plot_reaching_task_targets(target, tsteps:int, T:int):

  # count targets
  n_targets = target.shape[0]

  # plot target coordinates throughout trial
  fig, axes = plt.subplots(n_targets,1, figsize=(6,6))
  for target_i in tuple(range(n_targets)):
    axes[target_i].plot(target[target_i,:,:])

    # legend
    axes[target_i].set_xticks([0, tsteps])
    axes[target_i].set_xticklabels([0, T])
    axes[target_i].set_ylabel("target" "\n" "coord (a.u.)", fontsize=9)
    axes[target_i].set_ylim([target.min()-0.01, target.max()+0.01])

  axes[-1].set_xlabel("time within a trial (secs)")
  plt.legend(["x-coord", "y-coord"], fontsize=9, frameon=False)
  fig.tight_layout()

# @title "force" task
def create_force_task_stimuli(tsteps:int, pulse_steps:int, n_targets:int=1, amplitude:float=1., twod:bool=False):
    """create a stimulus set

    Args:
      tsteps (int):
      pulse_steps (int):
      n_targets (int):
      amplitude (float)
      twod (bool):

    Returns:
      (np.array): array of (n_targets, pulse_steps, n_targets) stimuli
    """

    # create stimuli
    stimulus = np.zeros((n_targets, tsteps, n_targets))
    for j in range(n_targets):
        stimulus[j,:pulse_steps,j] = amplitude
    return stimulus

def plot_force_stimuli(stimulus, n_targets:int, tsteps:int, T:int):

  # plot target cue with "pulse_steps" duration
  # at the beginning of each trial
  stimulus_set = np.arange(0, n_targets, 1)
  fig, axes = plt.subplots(n_targets, 1, figsize=(30,9))
  for target in stimulus_set:

    # plot
    axes[target].imshow(stimulus[target,:,:].T, aspect=10, cmap="binary");

    # legend
    axes[target].set_yticks(stimulus_set)
    axes[target].set_yticklabels(stimulus_set, fontsize=9)
    axes[target].set_xticks([0, tsteps])
    axes[target].set_xticklabels([0, T])
    axes[target].set_ylabel("possible targets", fontsize=9)
  axes[-1].set_xlabel("time within a trial (secs)")
  fig.tight_layout()
  print("stimuli:")
  print(f"-a set of {stimulus.shape[0]} possible trial stimuli (panels)")
  print(f"-{stimulus.shape[1]} timesteps within a trial stimulus")
  print(f"-{stimulus.shape[2]} possible cued target in a trial stimulus")

def create_force_task_targets(tsteps, pulse_steps, targets:list=[1, 10], target_max:float=0.2):
  """exert force with an oscillatorily increasing and decreasing amplitude
  """
  n_targets = len(targets)
  rs = np.zeros(tsteps)
  traj = np.zeros((n_targets, tsteps, 1))
  rs[pulse_steps:] = np.ones(tsteps - pulse_steps) * target_max
  x_coord = np.linspace(-2*np.pi, 2*np.pi, tsteps, endpoint=False)
  freq = []
  for ix in range(n_targets):
    traj[ix,:,0] = rs * np.sin(targets[ix] * x_coord)
  return traj

def plot_force_task_targets(target, tsteps:int, T:int):

  # count targets
  n_targets = target.shape[0]

  # plot target coordinates throughout trial
  fig, axes = plt.subplots(n_targets,1, figsize=(6,6));
  for target_i in tuple(range(n_targets)):

    # plot
    axes[target_i].plot(target[target_i,:,:]);

    # legend
    axes[target_i].set_xticks([0, tsteps]);
    axes[target_i].set_xticklabels([0, T]);
    axes[target_i].set_ylabel("target" "\n" "coord (a.u.)", fontsize=9);
    axes[target_i].set_ylim([target.min()-0.01, target.max()+0.01]);
  axes[-1].set_xlabel("time within a trial (secs)");
  plt.legend(["x-coord", "y-coord"], fontsize=9, frameon=False);
  fig.tight_layout();
  
def plot_curser_trajectories(activity, decoder, color='k', alpha=1):
    """plot cursor trajectories for all targets/trials."""
    o_original = activity @ decoder.T
    
    # reconstruct trajectories from velocities
    pos_original = np.zeros(o_original.shape)
    for j in range(1, o_original.shape[1]):
        pos_original[:,j,:] = pos_original[:,j-1,:] + o_original[:,j,:]*dt
    
    # plot trajectories
    fig, ax = plt.subplots(figsize=(5, 5), dpi=96)
    for j in range(activity.shape[0]):
        plt.plot(pos_original[j,:,0], pos_original[j,:,1], color=color, alpha=alpha);
    ax.set_title('simulated reaching');
    ax.set_xlabel('x-position on screen');
    ax.set_ylabel('y-position on screen');
    
    return fig

def normalize_decoder(pert_decoder, orig_decoder):
    norm_orig = np.linalg.norm(orig_decoder, axis=1, keepdims=True)  # shape [2, 1]
    norm_pert = np.linalg.norm(pert_decoder, axis=1, keepdims=True)  # shape [2, 1]
    # Avoid divide-by-zero
    norm_pert[norm_pert == 0] = 1e-8
    normalized = pert_decoder * (norm_orig / norm_pert)
    return normalized\

def add_noise_to_feedback(feedback, alpha):
    sigma_original = np.std(feedback)
    noise = np.random.randn(*feedback.shape) * (alpha * sigma_original)
    return feedback + noise
# =============================================================================
# %%
# =============================================================================
"""## Create task 1: "reaching"
"""

# @title set parameters
# TODO: create dictionary

seed_id = 2                 # random seed for this simulation
np.random.seed(seed_id)

# time parameters
dt = 0.01                   # time discretization (secs, see paper table 1)
T = 2                       # trial duration (secs, see paper table 1)
time = np.arange(0, T, dt)
tsteps = len(time)          # number of time steps within a trial
pulse_length = int(0.2/dt)  # pulse length in number of timesteps

# network parameters
N = 800                     # RNN number of units
M = 800                    # pfc-rnn number of units
g = 1.5                     # RNN recurrent connection strengths (a.u)
p = 0.1                     # RNN connection probability
tau = 0.1                   # unit time constant tau (secs)
N_OUTPUT_UNITS = 2          # number of output units (2 for x and y)

# task parameters
targets = 6                 # number of reaching targets
stimulus_type = 'constant'  # constant, linear, normal
target_max = 0.2            # 0.2 or 0.01
n_learning1_trials = 80       # initial network learning
delta = 20.
relearning_trials = 80      # relearning
deltarec = 20.

# analyses parameters
manifold_trials = 50        # manifold calculation
reduced_dim = 10
# =============================================================================
# %%
# =============================================================================
# @title Create stimuli and targets

# create stimuli and plot
stimulus = create_reaching_task_stimuli(tsteps, pulse_length, twod=False)
plot_reaching_task_stimuli(stimulus, targets, tsteps, T)

# create target (targets x timesteps x 2D coordinates) and plot
target = create_reaching_task_targets(
    tsteps,
    pulse_length,
    n_targets=targets,
    stype=stimulus_type,
    target_max=target_max
    )
plot_reaching_task_targets(target, tsteps, T)
# =============================================================================
# %%
# =============================================================================
# @title build and train the network
# create and save M1 RNN encoder
z_init_pfc = np.random.randn(M)
W_init_pfc = np.random.randn(M, M)
W_projection = np.random.randn(N, M)
W_projection /= np.linalg.norm(W_projection, axis=1, keepdims=True)
context_init = W_projection @ z_init_pfc

m1_rnn = RNN(N=N, g=g, p=p, tau=tau, dt=dt, N_in=targets)

save_RNN(m1_rnn, savedir)

# create feedforward decoder from RNN to (x,y) output units
reaching_decoder = create_reaching_task_decoder(m1_rnn,
                                                n_output_units=N_OUTPUT_UNITS)

# create feedback weights from (x,y) output units back to RNN
reaching_feedback = get_feedback_weights(reaching_decoder)

# train and save the RNN encoder weights (learn the task)
reaching_loss = m1_rnn.relearn(n_learning1_trials, stimulus,
                                         pulse_length, reaching_decoder,
                                         reaching_feedback, target, delta=delta)
np.save(f'{savedir}W_stabilized_reaching', m1_rnn.W)
w1_reaching = m1_rnn.W.copy()

# get the RNN's manifold
manifold_out = get_manifold(m1_rnn)

# train the decoder
W_bci4, l4 = train_reaching_decoder(manifold_out["xi2"][:, :, :reduced_dim],
                                    target[:, pulse_length:, :],
                                    manifold_out["order"],
                                    n_output_units=N_OUTPUT_UNITS)

# transform
transformed = transform_reaching(m1_rnn, manifold_out,
                                 W_bci4, n_output_units=N_OUTPUT_UNITS)
# =============================================================================
# %%
# =============================================================================
# @title Save run data
# format and save data for this run
run_data = {
    'params':{
        'dt':dt,
        'T':T,
        'time':time,
        'tsteps':tsteps,
        'pulse_length':pulse_length,
        'manifold_trials':manifold_trials,
        'target_max':target_max,
        'stimulus_type':stimulus_type,
        'N':N,
        'tau':tau,
        'g':g,
        'p':p
        },
    'stimulus':stimulus,
    'target':target,
    'stabilizing':{
        'learning_trials':n_learning1_trials,
        'delta':delta,
        'decoder':reaching_decoder,
        'feedback':reaching_feedback,
        'stabilize_loss':reaching_loss
        },
    'manifold':{
        'activity':manifold_out["activity"],
        'activity2':manifold_out["activity_reshaped"],
        'xi':manifold_out["xi"],
        'xi2':manifold_out["xi2"],
        'cov':manifold_out["cov"],
        'ev':manifold_out["ev"],
        'evec':manifold_out["evec"],
        'pr':manifold_out["pr"],
        'order': manifold_out["order"]
        },
    'decoding':{
        'reduced_dim': reduced_dim,
        'weights': W_bci4,
        'loss':l4
        }
        }
np.save(f'{savedir}reaching_experiment_results', run_data)

# save manifold data separately
save_reaching_manifold(run_data, transformed)
# =============================================================================
# %%
# =============================================================================
class PFC_RNN(object):
    """
    Class implementing a recurrent network (not following Dale's law).

    Parameters:
    -----------
    * N: number of neurons
    * N_in: how many inputs can the network have
    * N_out: how many neurons are recorded by external device
    * g: recurrent coupling strength
    * p: connection probability
    * tau: neuron time constant
    * dt: set dt for simulation
    * delta: defines initial learning rate for FORCE
    * P_plastic: how many neurons are plastic in the recurrent network
    """
    def __init__(self, N=1000, g=1.5, p=0.1, tau=0.1, dt=0.01,
                 N_in=6, N_m1=800):
        # set parameters
        self.N = N
        self.g = g
        self.p = p
        self.K = int(p*N)
        self.tau = tau
        self.dt = dt
        self.N_m1 = N_m1

        # create recurrent W
        mask = np.random.rand(self.N,self.N)<self.p
        np.fill_diagonal(mask,np.zeros(self.N))
        self.mask = mask
        self.W = self.g / np.sqrt(self.K) * np.random.randn(self.N,self.N) * mask

        # create Win and Wout
        self._N_in = N_in
        self.W_in = (np.random.rand(self.N, self._N_in)-0.5)*2.
        # Initialize W_projection as a trainable attribute
        # We can start with a random projection, as before
        self.W_projection = np.random.randn(self.N_m1, self.N)
        self.W_projection /= np.linalg.norm(self.W_projection)

    @property
    def N_in(self):
        return self._N_in

    @N_in.setter
    def N_in(self, value):
        self._N_in = value
        self.W_in = (np.random.rand(self.N, self._N_in)-0.5)*2.

    def save(self,filename):
        np.savez(
            filename,
            N = self.N,
            K = self.K,
            tau = self.tau,
            g = self.g,
            p = self.p,
            dt = self.dt,
            W_in = self.W_in,
            W = self.W,
            N_in = self._N_in,
        )

    def load(self,filename):
        net = np.load(filename+'.npz')
        self.N = int(net['N'])
        self.dt = float(net['dt'])
        self.K = int(net['K'])
        self.tau = float(net['tau'])
        self.g = float(net['g'])
        self.p = float(net['p'])
        self.W_in = net['W_in']
        self.W = net['W']
        self._N_in = int(net['N_in'])

    def update_activation(self):
        self.z = np.tanh(self.r)

    def update_neurons(self,ext):   
        self.r = self.r + self.dt/self.tau * \
             (-self.r + np.dot(self.W, self.z) + np.dot(self.W_in,ext))

        self.update_activation()

    def simulate(self, T, ext=None, r0=None):

        # define time
        time = np.arange(0,T,self.dt)
        tsteps = int(T/self.dt)

        # create input in case no input is given
        if ext is None:
            ext = np.zeros((tsteps,self.N_in))

        # check if input has the right shape
        if ext.shape[0]!=tsteps or ext.shape[1]!=self.N_in:
            print('ERROR: stimulus shape should be (time x number of input nodes)')
            return

        # set initial condition
        if r0 is None:
            self.r = (np.random.rand(self.N)-0.5)*2.
        else:
            self.r = r0
        self.update_activation()

        # start simulation
        record_r = np.zeros((tsteps,self.N))
        record_r[0,:] = self.r
        for i in range(1,tsteps):
            self.update_neurons(ext=ext[i])
            # store activity
            record_r[i,:] = self.r
        return time, record_r, np.tanh(record_r)

    def relearn(self, trials, ext, ntstart, decoder, feedback, target, 
            delta_pfc=1., delta_m1=1e6, lr_proj=1e-6, 
            wplastic=None, M1_RNN=None):
        """
        Args
          self.z: RNN network's activation
          ext (np.array): stimuli (n_targets, n timesteps, n_targets)
          decoder (np.array): (N units, 2d coordinates) decoder weights
          feedback (np.array): (N units, 2d coordinates) feedback weights
          target: (n_targets, N timesteps, 2d coordinates) target coordinates

        Returns:

          loss (np.array): loss by trial
        """
        def sigmoid(x):
            return 1 / (1 + np.exp(-x))
        # get number of timesteps within trial
        tsteps = ext.shape[1]
        # set up learning
        if wplastic is None:
            self.W_plastic = [np.where(self.W[i,:]!=0)[0] for i in range(self.N)]
        else:
            self.W_plastic = wplastic
        self.P = [1./delta_pfc * np.eye(len(self.W_plastic[i])) for i in range(len(self.W_plastic))]
        
        # NEW: Initialize P for M1 recurrent weights
        if M1_RNN is not None:
            M1_RNN.W_plastic = [np.where(M1_RNN.W[i,:]!=0)[0] for i in range(M1_RNN.N)]
            M1_RNN.P = [1./delta_m1 * np.eye(len(M1_RNN.W_plastic[i])) for i in range(M1_RNN.N)]

        # create n trials of target indices chosen from 0 to 5
        order = np.random.choice(range(ext.shape[0]), trials, replace=True)

        # initialize calculated loss per trial
        record_loss = np.zeros(trials)
        
        # loop over trials
        for t in range(trials):

            # initialize loss
            loss = 0.
            self.r = (np.random.rand(self.N)-0.5)*2.
            self.update_activation()

            # loop over time
            for i in range(1,tsteps):

                # update units
                self.update_neurons(ext=ext[order[t],i])
                
                # In the PFC_RNN.relearn() method's main time loop
                if M1_RNN is not None:
                    # Calculate the gating signal from the PFC
                    pre_context = np.dot(self.W_projection, self.z)
                    gating_signal = sigmoid(pre_context)
                
                    # Update M1's internal state (NO context signal here)
                    M1_RNN.update_neurons(ext=ext[order[t], i])
                
                    # Apply the PFC gate to M1's output activation
                    M1_RNN.update_activation_gated(gating_signal)

                # learn
                if i > ntstart and i%2==0:

                    # decode network's predicted
                    # target coordinates
                    if M1_RNN is not None:
                        c = decoder @ M1_RNN.z
                    else:
                        c = decoder @ self.z
                    # calculate prediction error between
                    # decoded and true target coordinates (2,)
                    errc = c - target[order[t], i]

                    # calculate the error update assigned to each weight
                    err_m1 = feedback @ errc
                    err1 = self.W_projection.T @ err_m1

                    # calculate loss
                    loss += np.mean(err1**2)

                    # update plastic recurrent weights
                    for j in range(self.N):
                        z_plastic = self.z[self.W_plastic[j]]
                        pz = np.dot(self.P[j], z_plastic)
                        norm = (1. + np.dot(z_plastic.T,  pz))
                        self.P[j] -= np.outer(pz, pz)/norm
                    
                        # use error-transformed feedbacks to update
                        # plastic weights
                        self.W[j, self.W_plastic[j]] -= err1[j] * pz / norm
                    # *** NEW: UPDATE PROJECTION WEIGHTS (using err_m1) ***
                    # This is a simple gradient descent step (Hebbian-style rule)
                    # It uses the M1 error and the pre-synaptic PFC activity (self.z)
                    dW_projection = np.outer(err_m1, self.z)
                    self.W_projection -= lr_proj * dW_projection
                    if M1_RNN is not None:
                        for j in range(M1_RNN.N):
                            z_plastic_m1 = M1_RNN.z[M1_RNN.W_plastic[j]]
                            pz_m1 = np.dot(M1_RNN.P[j], z_plastic_m1)
                            norm_m1 = (1. + np.dot(z_plastic_m1.T, pz_m1))
                            M1_RNN.P[j] -= np.outer(pz_m1, pz_m1) / norm_m1
                            # Use the M1-specific error (err_m1) to update M1's weights
                            M1_RNN.W[j, M1_RNN.W_plastic[j]] -= err_m1[j] * pz_m1 / norm_m1
            # tape loss
            record_loss[t] = loss
            print('Loss in Trial %d is %.5f'%(t+1,loss))
        return record_loss

    def calculate_manifold(self, trials, ext, ntstart):
        tsteps = ext.shape[1]
        T = self.dt*tsteps
        points = (tsteps-ntstart)
        activity = np.zeros((points*trials,self.N))
        order = np.random.choice(range(ext.shape[0]),trials,replace=True)
        for t in range(trials):
            time, r, z = self.simulate(T,ext[order[t]])
            activity[t*points:(t+1)*points,:] = z[ntstart:,:]
        cov = np.cov(activity.T)
        ev,evec = np.linalg.eig(cov)
        pr = np.round(np.sum(ev.real)**2/np.sum(ev.real**2)).astype(int)
        xi = activity @ evec.real
        return activity,cov,ev.real,evec.real,pr,xi,order
# =============================================================================
# %%
# =============================================================================
# @title build and train the network
# create and save PFC RNN encoder
# create feedforward decoder from RNN to (x,y) output units
# pfc_rnn = PFC_RNN(N=M, g=g, p=p, tau=tau, dt=dt, N_in=targets)
# # Assume M1_RNN, PFC_RNN classes, and W_PFC (N_M1, N_PFC)
# W_mp = np.random.randn(M, N)
# W_mp /= np.linalg.norm(W_mp, axis=1, keepdims=True)

# pfc_feedback = W_mp @ reaching_feedback

# # train and save the RNN encoder weights (learn the task)
# adaptation_loss = pfc_rnn.relearn(n_learning1_trials, stimulus,
#                                          pulse_length, reaching_decoder,
#                                          pfc_feedback, target, delta=delta, M1_RNN=m1_rnn)
# np.save(f'{savedir}PFC_W_stabilized_reaching', pfc_rnn.W)

# # get the RNN's manifold
# manifold_out_pfc = get_manifold(pfc_rnn)
# =============================================================================
# %%
# =============================================================================
"""### simulate reaching"""
# simulate reaching
trajectories = simulate_reaching(savedir, dt)

run_data = np.load(f'{savedir}reaching_experiment_results.npy', allow_pickle=True).item()
manifold_out = run_data["manifold"]
fig = plot_variance_explained(manifold_out['ev'])


xi2 = manifold_out["xi2"]
pc1 = xi2[:, :, 0]
pc2 = xi2[:, :, 1]
n_trials = xi2.shape[0]

order = manifold_out["order"]
colors = ['r', 'g', 'b', 'c', 'm', 'y']
plt.figure(figsize=(4, 4))
for trial in range(n_trials):
    t_idx = order[trial] % len(colors)
    plt.plot(pc1[trial], pc2[trial], color=colors[t_idx], alpha=0.8)
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.title('Neural Manifold: Trajectories by Target')
plt.tight_layout()
plt.show()

pc1 = xi2[:, :, 0]
pc2 = xi2[:, :, 1]
pc3 = xi2[:, :, 2]
from mpl_toolkits.mplot3d import Axes3D

order = manifold_out["order"]
colors = ['r', 'g', 'b', 'c', 'm', 'y']  # adjust for number of targets

fig = plt.figure(figsize=(5, 5))
ax = fig.add_subplot(111, projection='3d')

for trial in range(xi2.shape[0]):
    t_idx = order[trial] % len(colors)
    ax.plot(pc1[trial], pc2[trial], pc3[trial], color=colors[t_idx], alpha=0.8)

ax.set_xlabel('PC1')
ax.set_ylabel('PC2')
ax.set_zlabel('PC3')
ax.set_title('Neural Manifold: Trajectories in First 3 PCs')
plt.tight_layout()
plt.show()
# =============================================================================
# %%
# =============================================================================

run_data = np.load(f'{savedir}reaching_experiment_results.npy', allow_pickle=True).item()

manifold_out = run_data['manifold']
decoder_orig = run_data['decoding']['weights']
activity2 = manifold_out['activity2']  # [trials, tsteps, N]
xi2 = manifold_out['xi2']              # [trials, tsteps, n_components]
order = manifold_out['order']
target = run_data['target']
stimulus = run_data['stimulus']
pulse_length = run_data['params']['pulse_length']
N = run_data['params']['N']
reduced_dim = run_data['decoding']['reduced_dim']
T = run_data['params']['T']
dt = run_data['params']['dt']
n_learning_trials = run_data['stabilizing']['learning_trials']
delta = run_data['stabilizing']['delta']

fig = plot_curser_trajectories(manifold_out['activity2'], transformed)
fig.savefig(f'{savedir}original.svg', dpi=300)
from scipy.linalg import null_space

evec = manifold_out['evec']            # [N, N]
top_pc_basis = evec[:, :reduced_dim]   # [N, reduced_dim]

# 2. WITHIN-MANIFOLD: Random linear combination of top PCs
np.random.seed(42)
wmp_decoder = np.random.randn(2, reduced_dim) @ top_pc_basis.T
wmp_decoder = normalize_decoder(wmp_decoder, transformed)

fig = plot_curser_trajectories(manifold_out['activity2'], wmp_decoder, color='darkred')
fig.savefig(f'{savedir}wmp.svg', dpi=300)
# 3. OUTSIDE-MANIFOLD: Decoder in null space of manifold
null_basis = null_space(top_pc_basis.T)  # [N, N-reduced_dim]
# 2 random directions in null space
omp_decoder = np.random.randn(2, null_basis.shape[1]) @ null_basis.T
omp_decoder = normalize_decoder(omp_decoder, transformed)

fig = plot_curser_trajectories(manifold_out['activity2'], omp_decoder, color='darkblue')
fig.savefig(f'{savedir}omp.svg', dpi=300)

wmp_feedback = get_feedback_weights(wmp_decoder)
omp_feedback = get_feedback_weights(omp_decoder)


reaching_network_wmp = RNN(N=N, g=g, p=p, tau=tau, dt=dt, N_in=targets)
reaching_network_wmp.W = np.load(f'{savedir}W_stabilized_reaching.npy')


# Within-manifold retraining
loss_wmp = reaching_network_wmp.relearn(
    n_learning_trials, stimulus, pulse_length,
    wmp_decoder, wmp_feedback, target, delta=delta
)

manifold_out_wmp = get_manifold(reaching_network_wmp)

fig = plot_curser_trajectories(manifold_out_wmp['activity_reshaped'], wmp_decoder, color='darkred')
fig.savefig(f'{savedir}wmp-wo_noise.svg', dpi=300)

reaching_network_omp = RNN(N=N, g=g, p=p, tau=tau, dt=dt, N_in=targets)
reaching_network_omp.W = np.load(f'{savedir}W_stabilized_reaching.npy')
# Outside-manifold retraining
loss_omp = reaching_network_omp.relearn(
    n_learning_trials, stimulus, pulse_length,
    omp_decoder, omp_feedback, target, delta=delta
)

manifold_out_omp = get_manifold(reaching_network_omp)

fig = plot_curser_trajectories(manifold_out_omp['activity_reshaped'], omp_decoder, color='darkblue')
fig.savefig(f'{savedir}omp_wo_noise.svg', dpi=300)
# =============================================================================
# %%
# =============================================================================

def get_controlled_manifold(pfc_rnn, m1_rnn, W_projection, trials, ext, ntstart):
    """
    Simulates the coupled PFC-M1 system using multiplicative gating and 
    computes the manifold of the resulting M1 activity.
    """
    np.random.seed(42)
    def sigmoid(x):
        return 1 / (1 + np.exp(-x))

    tsteps = ext.shape[1]
    T = m1_rnn.dt * tsteps
    points = (tsteps - ntstart)
    
    m1_activity = np.zeros((points * trials, m1_rnn.N))
    order = np.random.choice(range(ext.shape[0]), trials, replace=True)

    for t in range(trials):
        # --- Start of new trial-by-trial simulation logic ---
        # Reset initial states for both networks
        pfc_rnn.r = (np.random.rand(pfc_rnn.N) - 0.5) * 2.
        pfc_rnn.update_activation()
        m1_rnn.r = (np.random.rand(m1_rnn.N) - 0.5) * 2.
        m1_rnn.update_activation()
        
        # Array to record M1 activity for this trial
        trial_m1_z = np.zeros((tsteps, m1_rnn.N))

        # Loop through time within the trial
        for i in range(1, tsteps):
            # 1. Update PFC-RNN (standard update)
            pfc_rnn.update_neurons(ext=ext[order[t], i])
            
            # 2. Calculate the gating signal from the PFC
            pre_context = W_projection @ pfc_rnn.z
            gating_signal = sigmoid(pre_context)

            # 3. Update M1-RNN's internal state (NO context)
            m1_rnn.update_neurons(ext=ext[order[t], i])

            # 4. Apply the multiplicative gate to M1's output
            m1_rnn.update_activation_gated(gating_signal)
            
            # 5. Record the final, gated M1 activation
            trial_m1_z[i, :] = m1_rnn.z
        
        # Store the relevant part of the activity
        m1_activity[t*points:(t+1)*points, :] = trial_m1_z[ntstart:, :]
        # --- End of new simulation logic ---

    # 5. Compute the manifold from the controlled M1 activity
    cov = np.cov(m1_activity.T)
    ev, evec = np.linalg.eig(cov)
    pr = np.round(np.sum(ev.real)**2 / np.sum(ev.real**2)).astype(int)
    xi = m1_activity @ evec.real
    
    # Reshape for plotting and return
    activity_reshaped = m1_activity.reshape(trials, -1, m1_rnn.N)
    xi2 = xi.reshape(trials, -1, m1_rnn.N)
    
    return {"xi2":xi2, "order":order, "xi":xi, "cov":cov, "ev":ev, "evec":evec, "pr":pr,"activity":m1_activity, "activity_reshaped":activity_reshaped}

import copy
pfc_rnn_wmp = PFC_RNN(N=100, g=g, p=p, tau=tau, dt=dt, N_in=targets, N_m1=800)
m1_rnn_wmp = copy.deepcopy(m1_rnn)
# W_projection = create_population_targeted_projection(M, N)
# train and save the RNN encoder weights (learn the task)
adaptation_loss_wmp = pfc_rnn_wmp.relearn(80, stimulus,
                                         pulse_length, wmp_decoder,
                                         wmp_feedback, target, delta_pfc=20, delta_m1=5,
                                         M1_RNN=m1_rnn_wmp, lr_proj=1e-6)

manifold_out_pfc_wmp = get_controlled_manifold(
    pfc_rnn=pfc_rnn_wmp,
    m1_rnn=m1_rnn_wmp,
    W_projection=pfc_rnn_wmp.W_projection,
    trials=manifold_trials,
    ext=stimulus,
    ntstart=pulse_length
)


fig = plot_curser_trajectories(manifold_out_pfc_wmp['activity_reshaped'], wmp_decoder, color='red')
fig.savefig(f'{savedir}6.svg', dpi=300)

cursor_vel_wmp = manifold_out_pfc_wmp['activity_reshaped'] @ wmp_decoder.T
mse_wmp = np.mean((cursor_vel_wmp - target[manifold_out_pfc_wmp['order'], pulse_length:])**2)
#######################################
pfc_rnn_omp = PFC_RNN(N=400, g=g, p=p, tau=tau, dt=dt, N_in=targets, N_m1=800)
m1_rnn_omp = copy.deepcopy(m1_rnn)
# W_projection = create_population_targeted_projection(M, N)
# train and save the RNN encoder weights (learn the task)
adaptation_loss_omp = pfc_rnn_omp.relearn(n_learning1_trials, stimulus,
                                         pulse_length, omp_decoder,
                                         omp_feedback, target, delta_pfc=20, delta_m1=1e8,
                                         M1_RNN=m1_rnn_omp, lr_proj=1e-7)

manifold_out_pfc_omp = get_controlled_manifold(
    pfc_rnn=pfc_rnn_omp,
    m1_rnn=m1_rnn_omp,
    W_projection=pfc_rnn_omp.W_projection,
    trials=manifold_trials,
    ext=stimulus,
    ntstart=pulse_length
)

fig = plot_curser_trajectories(manifold_out_pfc_omp['activity_reshaped'], omp_decoder, color='darkred')

cursor_vel_omp = manifold_out_pfc_omp['activity_reshaped'] @ omp_decoder.T
mse_omp = np.mean((cursor_vel_omp - target[manifold_out_pfc_omp['order'], pulse_length:])**2)


# =============================================================================
# %%
# =============================================================================
"""## Create task 2: "force"

Don't hesitate to use some of the utils functions that have been implemented for this tasks under the "force" task section.

Can you think of other tasks to test?
"""

tsteps = 200
pulse_length = 20
T = 2
targets_list = [1, -1]
n_targets = len(targets_list)

stimulus = create_force_task_stimuli(tsteps, pulse_length, n_targets=n_targets)
plot_force_stimuli(stimulus, n_targets, tsteps, T)

target = create_force_task_targets(tsteps, pulse_length, targets=targets_list, target_max=0.2)
plot_force_task_targets(target, tsteps, T)

N_OUTPUT_UNITS_FORCE = 1

force_network = RNN(N=N, g=g, p=p, tau=tau, dt=dt, N_in=n_targets)
save_RNN(force_network, savedir)

force_decoder = create_reaching_task_decoder(force_network, N_OUTPUT_UNITS_FORCE)

force_feedback = get_feedback_weights(force_decoder)

force_loss = force_network.relearn(
    n_learning1_trials, stimulus, pulse_length,
    force_decoder, force_feedback, target, delta=delta
)
np.save(f'{savedir}W_stabilized_force', force_network.W)
w1_force = force_network.W.copy()

manifold_out_force = get_manifold(force_network)

W_bci4_force, l4_force = train_force_exertion_decoder(
    manifold_out_force["xi2"][:, :, :reduced_dim],
    target[:, pulse_length:, :],
    manifold_out_force["order"],
    n_output=N_OUTPUT_UNITS_FORCE
)

transformed_force = transform_reaching(
    force_network, manifold_out_force, W_bci4_force, n_output_units=N_OUTPUT_UNITS_FORCE
)

force_outputs = []
n_targets = stimulus.shape[0]
for trial in range(n_targets):
    stim = stimulus[trial]
    time, r, z = force_network.simulate(T, ext=stim)
    force_outputs.append(z)  # (tsteps, N)

decoded_force = []
for trial in range(len(force_outputs)):
    output = force_outputs[trial] @ force_decoder.T
    decoded_force.append(output)
# xi2 shape: (manifold_trials, tsteps, N)
decoded_force = manifold_out_force["xi2"] @ force_decoder.T  # (manifold_trials, tsteps, 1)
time_short = time[:decoded_force[0].shape[0]]

plt.figure(figsize=(12,6))
for trial in range(len(decoded_force)):
    plt.plot(time_short, decoded_force[trial].squeeze(), alpha=0.7)
plt.xlabel('Time (s)')
plt.ylabel('Decoded Force Output')
plt.title('Simulated Force Output Trajectories')
plt.show()